{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file is to implement the designed feature-based similarity distance method\n",
    "# The cost function is defined as the square of discrepancy between DTW of two real time series and Euclidean of the\n",
    "# two time series in new feature space.\n",
    "# Date: 9/21/2016\n",
    "# Author: Zexi Chen(zchen22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import six.moves.cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples1 file contains 10000 time series, and each one has 23 time points\n",
    "# It is used as the training data\n",
    "fileObject1 = open('../theano/data/samples1','r')\n",
    "train_set = pickle.load(fileObject1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample2 file contains 10000 time series, same format\n",
    "# It is used as the validation set\n",
    "fileObject2 = open('../theano/data/samples2','r')\n",
    "valid_set = pickle.load(fileObject2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample3 file contains 10000 time series served as test data\n",
    "fileObject3 = open('../theano/data/samples3','r')\n",
    "test_set = pickle.load(fileObject3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37,  26,  33,  54,  39, 102, 112, 130, 171, 128, 125, 195, 196,\n",
       "        226, 225, 221, 184, 162, 143, 114, 103, 111, 111],\n",
       "       [ 96,  92,  98,  42,  99,  96, 114, 157, 152, 125, 149, 209, 237,\n",
       "        236, 237, 228, 171, 151, 126, 116, 108,  47, 107],\n",
       "       [ 36,  36,  29,  55,  42,  85,  94, 107, 109, 110, 111, 159, 229,\n",
       "        237, 240, 241, 133, 121, 104,  94,  96,  93,  87],\n",
       "       [ 32,  91,  88,  93,  92,  98,  95, 100, 110, 149, 196, 220, 231,\n",
       "        231, 231, 218, 156, 108,  92,  88,  91,  91,  89]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the training data\n",
    "train_set[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the array, concatenate two time series as one training instance\n",
    "train_set1 = numpy.reshape(train_set, (train_set.shape[0]/2, train_set.shape[1]*2))\n",
    "valid_set1 = numpy.reshape(valid_set, (valid_set.shape[0]/2, valid_set.shape[1]*2))\n",
    "test_set1 = numpy.reshape(test_set, (test_set.shape[0]/2, test_set.shape[1]*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37,  26,  33,  54,  39, 102, 112, 130, 171, 128, 125, 195, 196,\n",
       "        226, 225, 221, 184, 162, 143, 114, 103, 111, 111,  96,  92,  98,\n",
       "         42,  99,  96, 114, 157, 152, 125, 149, 209, 237, 236, 237, 228,\n",
       "        171, 151, 126, 116, 108,  47, 107],\n",
       "       [ 36,  36,  29,  55,  42,  85,  94, 107, 109, 110, 111, 159, 229,\n",
       "        237, 240, 241, 133, 121, 104,  94,  96,  93,  87,  32,  91,  88,\n",
       "         93,  92,  98,  95, 100, 110, 149, 196, 220, 231, 231, 231, 218,\n",
       "        156, 108,  92,  88,  91,  91,  89]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the reshaped train data. Every two consecutive rows are concatenated into one row with 46 dimension\n",
    "train_set1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dtw function\n",
    "def dtw(list1, list2, window = 1):\n",
    "    len1 = len(list1)\n",
    "    len2 = len(list2)\n",
    "    mat = [[float('inf') for x in range(len2 + 1)] for y in range(len1 + 1)]\n",
    "    mat[0][0] = 0\n",
    "    for i in range(1,len1 + 1):\n",
    "        if i - window <= 1:\n",
    "            start = 1\n",
    "        else:\n",
    "            start = i - window\n",
    "        \n",
    "        if i + window <= len2:\n",
    "            end = i + window\n",
    "        else:\n",
    "            end = len2\n",
    "        for j in range(start, end + 1):\n",
    "            cost = abs(float(list1[i - 1] - list2[j - 1]))\n",
    "            mat[i][j] = cost + min(mat[i-1][j], mat[i][j-1],mat[i-1][j-1])\n",
    "        \n",
    "    return mat[len1][len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define euclidean distance function \n",
    "def euclideanDist(list1,list2):\n",
    "    distance = 0\n",
    "    for x in range(len(list1)):\n",
    "        distance += pow((list1[x]-list2[x]),2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the dtw distance between the two time series in each row of the training data validation data and test data \n",
    "# the dtw is used in the cost function as the target value to minimize\n",
    "train_dtws = numpy.zeros((train_set1.shape[0],1))\n",
    "for i in range(train_set1.shape[0]):\n",
    "    train_dtws[i,0] = dtw(train_set1[i,0:23], train_set1[i,23:])\n",
    "    \n",
    "valid_dtws = numpy.zeros((valid_set1.shape[0],1))\n",
    "for i in range(valid_set1.shape[0]):\n",
    "    valid_dtws[i,0] = dtw(valid_set1[i,0:23], valid_set1[i,23:])\n",
    "    \n",
    "test_dtws = numpy.zeros((test_set1.shape[0],1))\n",
    "for i in range(test_set1.shape[0]):\n",
    "    test_dtws[i,0] = dtw(test_set1[i,0:23], test_set1[i,23:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the neural network model\n",
    "# start the tensorflow interaction interface\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of nerons in hidden layers\n",
    "numFeatureMaps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the flow graph. \n",
    "# create two variable placehold, x for the training features, \n",
    "# y for the labels(in this model it is the dtw distance between two time series)\n",
    "x = tf.placeholder(tf.float32, shape=[None, train_set1.shape[1]])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the weight matrix, initialize randomly \n",
    "# truncated_normal: output random values from a truncated normal distribution with value out of 2 sd dropped \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the training input to comform the CNN model [batch size, width, height, color channels]\n",
    "# x_ts = tf.to_float(x_ts)\n",
    "x_ts = tf.reshape(x, [-1,46,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the weight matrix and the bias\n",
    "# specify the filter size: [filter_width, filter_height, in_channels, out_channels]\n",
    "W_conv1 = weight_variable([23,1,1,numFeatureMaps])\n",
    "b_conv1 = bias_variable([numFeatureMaps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the model we use and set up the paratemers\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,23,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the non-linear function to transfer input to hidden layer\n",
    "h_conv1 = tf.nn.relu(conv2d(x_ts, W_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the cost function: (dtw-euclidean(timeseries1 new features, timeseries2 new features))^2\n",
    "h_conv1_flat = tf.reshape(h_conv1,[-1, 2 * numFeatureMaps])\n",
    "h_conv1_flat_diff = tf.square(tf.sub(h_conv1_flat[:,:numFeatureMaps],h_conv1_flat[:,numFeatureMaps:]))\n",
    "cost_function = tf.square(tf.sub(y_ ,tf.sqrt(tf.reduce_sum(h_conv1_flat_diff))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the training optimizer for the model\n",
    "train_step = tf.train.AdagradOptimizer(1e-3).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the model graph parameters\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, the mean error of the training data 3.90132e+06, vilidation data 4.00409e+06\n",
      "step 100, the mean error of the training data 1.10257e+06, vilidation data 1.13014e+06\n",
      "step 200, the mean error of the training data 815006, vilidation data 838264\n",
      "step 300, the mean error of the training data 654765, vilidation data 674945\n",
      "step 400, the mean error of the training data 543838, vilidation data 561621\n",
      "step 500, the mean error of the training data 461018, vilidation data 476806\n",
      "step 600, the mean error of the training data 396463, vilidation data 410696\n",
      "step 700, the mean error of the training data 344716, vilidation data 357721\n",
      "step 800, the mean error of the training data 302393, vilidation data 314275\n",
      "step 900, the mean error of the training data 267279, vilidation data 278180\n",
      "step 1000, the mean error of the training data 237824, vilidation data 247903\n",
      "step 1100, the mean error of the training data 212884, vilidation data 222243\n",
      "step 1200, the mean error of the training data 191591, vilidation data 200315\n",
      "step 1300, the mean error of the training data 173212, vilidation data 181315\n",
      "step 1400, the mean error of the training data 157060, vilidation data 164487\n",
      "step 1500, the mean error of the training data 136717, vilidation data 143000\n",
      "step 1600, the mean error of the training data 57077.6, vilidation data 60703.4\n",
      "step 1700, the mean error of the training data 49796.1, vilidation data 52926.8\n",
      "step 1800, the mean error of the training data 47047.2, vilidation data 49908.4\n",
      "step 1900, the mean error of the training data 45094.6, vilidation data 47749.2\n",
      "step 2000, the mean error of the training data 43498.3, vilidation data 45980.2\n",
      "step 2100, the mean error of the training data 42134.7, vilidation data 44462.1\n",
      "step 2200, the mean error of the training data 40938.6, vilidation data 43126.4\n",
      "step 2300, the mean error of the training data 39875.1, vilidation data 41942.2\n",
      "step 2400, the mean error of the training data 38916.7, vilidation data 40876.1\n",
      "step 2500, the mean error of the training data 38063, vilidation data 39918.4\n",
      "step 2600, the mean error of the training data 37302.2, vilidation data 39053.7\n",
      "step 2700, the mean error of the training data 36614, vilidation data 38261.3\n",
      "step 2800, the mean error of the training data 35992.6, vilidation data 37541.4\n",
      "step 2900, the mean error of the training data 35413.8, vilidation data 36862.7\n",
      "step 3000, the mean error of the training data 34870.8, vilidation data 36224.7\n",
      "step 3100, the mean error of the training data 34331.1, vilidation data 35582.5\n",
      "step 3200, the mean error of the training data 33776.6, vilidation data 34924.7\n",
      "step 3300, the mean error of the training data 32789.8, vilidation data 33693.2\n",
      "step 3400, the mean error of the training data 30115.1, vilidation data 29890.8\n",
      "step 3500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 3600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 3700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 3800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 3900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 4900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 5900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 6900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 7900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 8900, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9000, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9100, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9200, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9300, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9400, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9500, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9600, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9700, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9800, the mean error of the training data 30111.6, vilidation data 29841\n",
      "step 9900, the mean error of the training data 30111.6, vilidation data 29841\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "train_error = []\n",
    "valid_error = []\n",
    "training_epochs = 10000\n",
    "best_valid_error = numpy.inf\n",
    "for i in range(training_epochs):\n",
    "    train_step.run(feed_dict={x:train_set1, y_:train_dtws})\n",
    "    if i%100 == 0:\n",
    "        train_err = cost_function.eval(feed_dict={x:train_set1, y_:train_dtws})\n",
    "        train_error.append(train_err.mean())\n",
    "        valid_err = cost_function.eval(feed_dict={x:valid_set1, y_:valid_dtws})\n",
    "        valid_error.append(valid_err.mean())\n",
    "        print(\"step %d, the mean error of the training data %g, vilidation data %g\"%(i, train_error[-1], valid_error[-1]))\n",
    "        #print h_conv1_flat.eval(feed_dict={x:test_set1})\n",
    "        if valid_error[-1] < best_valid_error * 0.995:\n",
    "            best_valid_error = valid_error[-1]\n",
    "            \n",
    "            saver = tf.train.Saver({\"W_0\": W_conv1, \"b_0\":b_conv1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xaxis = range(training_epochs/100)\n",
    "plt.plot(Xaxis, train_error, 'r',label='train error')\n",
    "plt.plot(Xaxis, valid_error, 'g',label='validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: model_param/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "  save_path = saver.save(sess, \"model_param/model.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-95e7937a6aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'b_0' is not defined"
     ]
    }
   ],
   "source": [
    "Weights = tf.Variable(-1.0, validate_shape=False, name='W_0')\n",
    "b0 = tf.Variable(-1.0, validate_shape=False, name='b_0')\n",
    "with tf.Session as session:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
