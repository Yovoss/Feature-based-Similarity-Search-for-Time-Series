{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file is to implement the designed feature-based similarity distance method\n",
    "# The cost function is defined as the square of discrepancy between DTW of two real time series and Euclidean of the\n",
    "# two time series in new feature space.\n",
    "# Date: 9/21/2016\n",
    "# Author: Zexi Chen(zchen22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import six.moves.cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples1 file contains 10000 time series, and each one has 23 time points\n",
    "# It is used as the training data\n",
    "fileObject1 = open('../theano/data/samples1','r')\n",
    "train_set = pickle.load(fileObject1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample2 file contains 10000 time series, same format\n",
    "# It is used as the validation set\n",
    "fileObject2 = open('../theano/data/samples2','r')\n",
    "valid_set = pickle.load(fileObject2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample3 file contains 10000 time series served as test data\n",
    "fileObject3 = open('../theano/data/samples3','r')\n",
    "test_set = pickle.load(fileObject3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37,  26,  33,  54,  39, 102, 112, 130, 171, 128, 125, 195, 196,\n",
       "        226, 225, 221, 184, 162, 143, 114, 103, 111, 111],\n",
       "       [ 96,  92,  98,  42,  99,  96, 114, 157, 152, 125, 149, 209, 237,\n",
       "        236, 237, 228, 171, 151, 126, 116, 108,  47, 107],\n",
       "       [ 36,  36,  29,  55,  42,  85,  94, 107, 109, 110, 111, 159, 229,\n",
       "        237, 240, 241, 133, 121, 104,  94,  96,  93,  87],\n",
       "       [ 32,  91,  88,  93,  92,  98,  95, 100, 110, 149, 196, 220, 231,\n",
       "        231, 231, 218, 156, 108,  92,  88,  91,  91,  89]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the training data\n",
    "train_set[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the array, concatenate two time series as one training instance\n",
    "train_set1 = numpy.reshape(train_set, (train_set.shape[0]/2, train_set.shape[1]*2))\n",
    "valid_set1 = numpy.reshape(valid_set, (valid_set.shape[0]/2, valid_set.shape[1]*2))\n",
    "test_set1 = numpy.reshape(test_set, (test_set.shape[0]/2, test_set.shape[1]*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 37,  26,  33,  54,  39, 102, 112, 130, 171, 128, 125, 195, 196,\n",
       "        226, 225, 221, 184, 162, 143, 114, 103, 111, 111,  96,  92,  98,\n",
       "         42,  99,  96, 114, 157, 152, 125, 149, 209, 237, 236, 237, 228,\n",
       "        171, 151, 126, 116, 108,  47, 107],\n",
       "       [ 36,  36,  29,  55,  42,  85,  94, 107, 109, 110, 111, 159, 229,\n",
       "        237, 240, 241, 133, 121, 104,  94,  96,  93,  87,  32,  91,  88,\n",
       "         93,  92,  98,  95, 100, 110, 149, 196, 220, 231, 231, 231, 218,\n",
       "        156, 108,  92,  88,  91,  91,  89]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the reshaped train data. Every two consecutive rows are concatenated into one row with 46 dimension\n",
    "train_set1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dtw function\n",
    "def dtw(list1, list2, window = 1):\n",
    "    len1 = len(list1)\n",
    "    len2 = len(list2)\n",
    "    mat = [[float('inf') for x in range(len2 + 1)] for y in range(len1 + 1)]\n",
    "    mat[0][0] = 0\n",
    "    for i in range(1,len1 + 1):\n",
    "        if i - window <= 1:\n",
    "            start = 1\n",
    "        else:\n",
    "            start = i - window\n",
    "        \n",
    "        if i + window <= len2:\n",
    "            end = i + window\n",
    "        else:\n",
    "            end = len2\n",
    "        for j in range(start, end + 1):\n",
    "            cost = abs(float(list1[i - 1] - list2[j - 1]))\n",
    "            mat[i][j] = cost + min(mat[i-1][j], mat[i][j-1],mat[i-1][j-1])\n",
    "        \n",
    "    return mat[len1][len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define euclidean distance function \n",
    "def euclideanDist(list1,list2):\n",
    "    distance = 0\n",
    "    for x in range(len(list1)):\n",
    "        distance += pow((list1[x]-list2[x]),2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the dtw distance between the two time series in each row of the training data validation data and test data \n",
    "# the dtw is used in the cost function as the target value to minimize\n",
    "train_dtws = numpy.zeros((train_set1.shape[0],1))\n",
    "for i in range(train_set1.shape[0]):\n",
    "    train_dtws[i,0] = dtw(train_set1[i,0:23], train_set1[i,23:])**2\n",
    "    \n",
    "valid_dtws = numpy.zeros((valid_set1.shape[0],1))\n",
    "for i in range(valid_set1.shape[0]):\n",
    "    valid_dtws[i,0] = dtw(valid_set1[i,0:23], valid_set1[i,23:])**2\n",
    "    \n",
    "test_dtws = numpy.zeros((test_set1.shape[0],1))\n",
    "for i in range(test_set1.shape[0]):\n",
    "    test_dtws[i,0] = dtw(test_set1[i,0:23], test_set1[i,23:])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f388dd707d0>> ignored\n"
     ]
    }
   ],
   "source": [
    "# build the neural network model\n",
    "# start the tensorflow interaction interface\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of nerons in hidden layers\n",
    "numFeatureMaps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate the flow graph. \n",
    "# create two variable placehold, x for the training features, \n",
    "# y for the labels(in this model it is the dtw distance between two time series)\n",
    "x = tf.placeholder(tf.float32, shape=[None, train_set1.shape[1]])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the weight matrix, initialize randomly \n",
    "# truncated_normal: output random values from a truncated normal distribution with value out of 2 sd dropped \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the training input to comform the CNN model [batch size, width, height, color channels]\n",
    "# x_ts = tf.to_float(x_ts)\n",
    "x_ts = tf.reshape(x, [-1,46,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the weight matrix and the bias\n",
    "# specify the filter size: [filter_width, filter_height, in_channels, out_channels]\n",
    "W_conv1 = weight_variable([23,1,1,numFeatureMaps])\n",
    "b_conv1 = bias_variable([numFeatureMaps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the model we use and set up the paratemers\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,23,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the non-linear function to transfer input to hidden layer\n",
    "h_conv1 = tf.nn.relu(conv2d(x_ts, W_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the cost function: (dtw-euclidean(timeseries1 new features, timeseries2 new features))^2\n",
    "h_conv1_flat = tf.reshape(h_conv1,[-1, 2 * numFeatureMaps])\n",
    "h_conv1_flat_diff = tf.square(tf.sub(h_conv1_flat[:,:numFeatureMaps],h_conv1_flat[:,numFeatureMaps:]))\n",
    "cost_function = tf.reduce_mean(tf.square(tf.sub(y_ ,tf.reduce_sum(h_conv1_flat_diff, 1, keep_dims=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the training optimizer for the model\n",
    "train_step = tf.train.AdamOptimizer(1e-1).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the model graph parameters\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, the mean error of the training data 2.66132e+10, vilidation data 2.72613e+10\n",
      "step 100, the mean error of the training data 2.3087e+09, vilidation data 3.66434e+09\n",
      "step 200, the mean error of the training data 2.06515e+09, vilidation data 3.50882e+09\n",
      "step 300, the mean error of the training data 2.03628e+09, vilidation data 3.60167e+09\n",
      "step 400, the mean error of the training data 1.87544e+09, vilidation data 3.52416e+09\n",
      "step 500, the mean error of the training data 1.85614e+09, vilidation data 3.52477e+09\n",
      "step 600, the mean error of the training data 1.80696e+09, vilidation data 3.59799e+09\n",
      "step 700, the mean error of the training data 1.7415e+09, vilidation data 3.58178e+09\n",
      "step 800, the mean error of the training data 1.80031e+09, vilidation data 3.68136e+09\n",
      "step 900, the mean error of the training data 1.67779e+09, vilidation data 3.63936e+09\n",
      "step 1000, the mean error of the training data 1.66066e+09, vilidation data 3.62467e+09\n",
      "step 1100, the mean error of the training data 1.65407e+09, vilidation data 3.65038e+09\n",
      "step 1200, the mean error of the training data 1.7402e+09, vilidation data 3.89611e+09\n",
      "step 1300, the mean error of the training data 1.69694e+09, vilidation data 3.77665e+09\n",
      "step 1400, the mean error of the training data 1.93718e+09, vilidation data 3.97057e+09\n",
      "step 1500, the mean error of the training data 1.66573e+09, vilidation data 3.8516e+09\n",
      "step 1600, the mean error of the training data 1.64975e+09, vilidation data 3.86057e+09\n",
      "step 1700, the mean error of the training data 1.65982e+09, vilidation data 3.95251e+09\n",
      "step 1800, the mean error of the training data 1.62315e+09, vilidation data 3.90212e+09\n",
      "step 1900, the mean error of the training data 1.62105e+09, vilidation data 3.9557e+09\n",
      "step 2000, the mean error of the training data 1.68654e+09, vilidation data 4.16125e+09\n",
      "step 2100, the mean error of the training data 1.62057e+09, vilidation data 4.03718e+09\n",
      "step 2200, the mean error of the training data 1.64113e+09, vilidation data 3.99982e+09\n",
      "step 2300, the mean error of the training data 1.85163e+09, vilidation data 4.02196e+09\n",
      "step 2400, the mean error of the training data 1.62412e+09, vilidation data 4.06071e+09\n",
      "step 2500, the mean error of the training data 1.62243e+09, vilidation data 4.14472e+09\n",
      "step 2600, the mean error of the training data 1.76656e+09, vilidation data 4.3981e+09\n",
      "step 2700, the mean error of the training data 1.65597e+09, vilidation data 4.22207e+09\n",
      "step 2800, the mean error of the training data 1.58819e+09, vilidation data 4.28354e+09\n",
      "step 2900, the mean error of the training data 1.59412e+09, vilidation data 4.34567e+09\n",
      "step 3000, the mean error of the training data 1.67896e+09, vilidation data 4.37297e+09\n",
      "step 3100, the mean error of the training data 1.64573e+09, vilidation data 4.2888e+09\n",
      "step 3200, the mean error of the training data 1.63235e+09, vilidation data 4.38366e+09\n",
      "step 3300, the mean error of the training data 1.86005e+09, vilidation data 4.42525e+09\n",
      "step 3400, the mean error of the training data 1.64514e+09, vilidation data 4.38651e+09\n",
      "step 3500, the mean error of the training data 1.65071e+09, vilidation data 4.38911e+09\n",
      "step 3600, the mean error of the training data 1.64005e+09, vilidation data 4.55052e+09\n",
      "step 3700, the mean error of the training data 2.04245e+09, vilidation data 4.71487e+09\n",
      "step 3800, the mean error of the training data 2.32433e+09, vilidation data 4.71642e+09\n",
      "step 3900, the mean error of the training data 1.68968e+09, vilidation data 4.48567e+09\n",
      "step 4000, the mean error of the training data 1.75312e+09, vilidation data 4.84861e+09\n",
      "step 4100, the mean error of the training data 1.64337e+09, vilidation data 4.84781e+09\n",
      "step 4200, the mean error of the training data 1.59371e+09, vilidation data 4.74769e+09\n",
      "step 4300, the mean error of the training data 1.67277e+09, vilidation data 4.92018e+09\n",
      "step 4400, the mean error of the training data 1.62352e+09, vilidation data 4.9713e+09\n",
      "step 4500, the mean error of the training data 1.60615e+09, vilidation data 5.13374e+09\n",
      "step 4600, the mean error of the training data 1.61549e+09, vilidation data 5.09798e+09\n",
      "step 4700, the mean error of the training data 1.63633e+09, vilidation data 5.08338e+09\n",
      "step 4800, the mean error of the training data 1.59754e+09, vilidation data 5.32098e+09\n",
      "step 4900, the mean error of the training data 1.58517e+09, vilidation data 5.23734e+09\n",
      "step 5000, the mean error of the training data 1.63059e+09, vilidation data 5.28504e+09\n",
      "step 5100, the mean error of the training data 1.60981e+09, vilidation data 5.14224e+09\n",
      "step 5200, the mean error of the training data 1.62039e+09, vilidation data 5.18427e+09\n",
      "step 5300, the mean error of the training data 1.61407e+09, vilidation data 5.31942e+09\n",
      "step 5400, the mean error of the training data 1.68889e+09, vilidation data 5.1343e+09\n",
      "step 5500, the mean error of the training data 1.57694e+09, vilidation data 5.34635e+09\n",
      "step 5600, the mean error of the training data 1.57681e+09, vilidation data 5.33258e+09\n",
      "step 5700, the mean error of the training data 1.66645e+09, vilidation data 5.15799e+09\n",
      "step 5800, the mean error of the training data 1.6749e+09, vilidation data 5.28861e+09\n",
      "step 5900, the mean error of the training data 1.57356e+09, vilidation data 5.55901e+09\n",
      "step 6000, the mean error of the training data 1.56296e+09, vilidation data 5.5278e+09\n",
      "step 6100, the mean error of the training data 1.59299e+09, vilidation data 5.70479e+09\n",
      "step 6200, the mean error of the training data 1.55453e+09, vilidation data 5.55318e+09\n",
      "step 6300, the mean error of the training data 1.58634e+09, vilidation data 5.55608e+09\n",
      "step 6400, the mean error of the training data 1.61604e+09, vilidation data 5.54962e+09\n",
      "step 6500, the mean error of the training data 1.62001e+09, vilidation data 5.57405e+09\n",
      "step 6600, the mean error of the training data 1.55775e+09, vilidation data 5.9643e+09\n",
      "step 6700, the mean error of the training data 1.55236e+09, vilidation data 5.89995e+09\n",
      "step 6800, the mean error of the training data 1.59689e+09, vilidation data 5.83701e+09\n",
      "step 6900, the mean error of the training data 1.59056e+09, vilidation data 6.06149e+09\n",
      "step 7000, the mean error of the training data 1.674e+09, vilidation data 6.42183e+09\n",
      "step 7100, the mean error of the training data 1.56796e+09, vilidation data 6.32899e+09\n",
      "step 7200, the mean error of the training data 1.55779e+09, vilidation data 6.44966e+09\n",
      "step 7300, the mean error of the training data 1.56323e+09, vilidation data 6.68487e+09\n",
      "step 7400, the mean error of the training data 1.57544e+09, vilidation data 6.69954e+09\n",
      "step 7500, the mean error of the training data 1.57613e+09, vilidation data 6.81962e+09\n",
      "step 7600, the mean error of the training data 1.81993e+09, vilidation data 7.73739e+09\n",
      "step 7700, the mean error of the training data 1.54511e+09, vilidation data 7.45179e+09\n",
      "step 7800, the mean error of the training data 1.79953e+09, vilidation data 8.47342e+09\n",
      "step 7900, the mean error of the training data 1.54184e+09, vilidation data 7.95784e+09\n",
      "step 8000, the mean error of the training data 1.6326e+09, vilidation data 8.04354e+09\n",
      "step 8100, the mean error of the training data 1.55171e+09, vilidation data 8.64008e+09\n",
      "step 8200, the mean error of the training data 1.5632e+09, vilidation data 8.60013e+09\n",
      "step 8300, the mean error of the training data 1.55071e+09, vilidation data 8.39189e+09\n",
      "step 8400, the mean error of the training data 1.59581e+09, vilidation data 8.24005e+09\n",
      "step 8500, the mean error of the training data 1.56509e+09, vilidation data 9.11392e+09\n",
      "step 8600, the mean error of the training data 1.54087e+09, vilidation data 8.9335e+09\n",
      "step 8700, the mean error of the training data 1.56289e+09, vilidation data 8.33707e+09\n",
      "step 8800, the mean error of the training data 1.5619e+09, vilidation data 8.40618e+09\n",
      "step 8900, the mean error of the training data 1.62363e+09, vilidation data 8.41085e+09\n",
      "step 9000, the mean error of the training data 1.59508e+09, vilidation data 9.23438e+09\n",
      "step 9100, the mean error of the training data 1.54704e+09, vilidation data 9.14003e+09\n",
      "step 9200, the mean error of the training data 1.57124e+09, vilidation data 9.14465e+09\n",
      "step 9300, the mean error of the training data 1.91002e+09, vilidation data 8.94638e+09\n",
      "step 9400, the mean error of the training data 1.52282e+09, vilidation data 9.00037e+09\n",
      "step 9500, the mean error of the training data 1.53713e+09, vilidation data 8.83642e+09\n",
      "step 9600, the mean error of the training data 1.55223e+09, vilidation data 9.29727e+09\n",
      "step 9700, the mean error of the training data 1.67161e+09, vilidation data 9.01378e+09\n",
      "step 9800, the mean error of the training data 1.52031e+09, vilidation data 8.9727e+09\n",
      "step 9900, the mean error of the training data 1.59335e+09, vilidation data 8.82928e+09\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "train_error = []\n",
    "valid_error = []\n",
    "training_epochs = 10000\n",
    "best_valid_error = numpy.inf\n",
    "for i in range(training_epochs):\n",
    "    train_step.run(feed_dict={x:train_set1, y_:train_dtws})\n",
    "    if i%100 == 0:\n",
    "        train_err = cost_function.eval(feed_dict={x:train_set1, y_:train_dtws})\n",
    "        train_error.append(train_err)\n",
    "        valid_err = cost_function.eval(feed_dict={x:valid_set1, y_:valid_dtws})\n",
    "        valid_error.append(valid_err)\n",
    "        print(\"step %d, the mean error of the training data %g, vilidation data %g\"%(i, train_error[-1], valid_error[-1]))\n",
    "        #print h_conv1_flat.eval(feed_dict={x:test_set1})\n",
    "        if valid_error[-1] < best_valid_error * 0.995:\n",
    "            W_best = sess.run(W_conv1)\n",
    "            b_best = sess.run(b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIGggiuUL3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f387c2924d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xaxis = [x * 100 for x in range(training_epochs/100)]\n",
    "plt.plot(Xaxis, train_error, 'r',label='train error')\n",
    "plt.plot(Xaxis, valid_error, 'g',label='validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: model_param/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "  save_path = saver.save(sess, \"model_param/model.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_0 = tf.Variable(-1.0, validate_shape=False, name='W_0')\n",
    "b_0 = tf.Variable(-1.0, validate_shape=False, name='b_0')\n",
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, \"model_param/model.ckpt\")\n",
    "    print(session.run(tf.all_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 468.],\n",
       "       [ 354.],\n",
       "       [ 751.],\n",
       "       ..., \n",
       "       [ 395.],\n",
       "       [ 250.],\n",
       "       [ 295.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# redo the method with re-scaled input data, and the input value ranges [0,1]\n",
    "train_set2 = train_set1/255.0\n",
    "valid_set2 = valid_set1/255.0\n",
    "test_set2 = test_set1/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14509804,  0.10196078,  0.12941176, ...,  0.42352941,\n",
       "         0.18431373,  0.41960784],\n",
       "       [ 0.14117647,  0.14117647,  0.11372549, ...,  0.35686275,\n",
       "         0.35686275,  0.34901961],\n",
       "       [ 0.87843137,  0.22352941,  0.91764706, ...,  0.24313725,\n",
       "         0.95294118,  0.38431373],\n",
       "       ..., \n",
       "       [ 0.1254902 ,  0.14117647,  0.1372549 , ...,  0.38823529,\n",
       "         0.34901961,  0.34509804],\n",
       "       [ 0.19607843,  0.16470588,  0.16862745, ...,  0.36862745,\n",
       "         0.36470588,  0.18039216],\n",
       "       [ 0.28235294,  0.17647059,  0.17254902, ...,  0.34509804,\n",
       "         0.35294118,  0.31764706]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the dtw distance between the two time series in each row of the training data validation data and test data \n",
    "# the dtw is used in the cost function as the target value to minimize\n",
    "train_dtws2 = numpy.zeros((train_set2.shape[0],1))\n",
    "for i in range(train_set2.shape[0]):\n",
    "    train_dtws2[i,0] = dtw(train_set2[i,0:23], train_set2[i,23:])\n",
    "    \n",
    "valid_dtws2 = numpy.zeros((valid_set2.shape[0],1))\n",
    "for i in range(valid_set2.shape[0]):\n",
    "    valid_dtws2[i,0] = dtw(valid_set2[i,0:23], valid_set2[i,23:])\n",
    "    \n",
    "test_dtws2 = numpy.zeros((test_set2.shape[0],1))\n",
    "for i in range(test_set2.shape[0]):\n",
    "    test_dtws2[i,0] = dtw(test_set2[i,0:23], test_set2[i,23:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the non-linear function to transfer input to hidden layer\n",
    "h_conv1 = tf.nn.sigmoid(conv2d(x_ts, W_conv1) + b_conv1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
