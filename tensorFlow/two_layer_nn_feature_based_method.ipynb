{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contained the feature-based similarity search method using two-layer neural network\n",
    "\n",
    "Created by Zexi Chen(zchen22)\n",
    "Date: Oct 2, 2016\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "import six.moves.cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from Feature_based_similarity_search import loadData, dtw, euclideanDist, NeuNet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data and rescale to 0-1\n",
    "train_set = loadData('../theano/data/samples1')\n",
    "valid_set = loadData('../theano/data/samples2')\n",
    "test_set = loadData('../theano/data/samples3')\n",
    "\n",
    "# reshape the array, concatenate two time series as one training instance\n",
    "train_set_reshape = numpy.reshape(train_set, (train_set.shape[0]/2, train_set.shape[1]*2))\n",
    "valid_set_reshape = numpy.reshape(valid_set, (valid_set.shape[0]/2, valid_set.shape[1]*2))\n",
    "test_set_reshape = numpy.reshape(test_set, (test_set.shape[0]/2, test_set.shape[1]*2))\n",
    "\n",
    "# re-scale input data\n",
    "train_set1 = train_set_reshape/255.0\n",
    "valid_set1 = valid_set_reshape/255.0\n",
    "test_set1 = test_set_reshape/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the squared dtw distance between the two time series in each row of the training data validation data and test data \n",
    "# the dtw is used in the cost function as the target value to minimize.\n",
    "train_dtws = numpy.zeros((train_set1.shape[0],1))\n",
    "for i in range(train_set1.shape[0]):\n",
    "    train_dtws[i,0] = dtw(train_set1[i,0:23], train_set1[i,23:])**2\n",
    "\n",
    "valid_dtws = numpy.zeros((valid_set1.shape[0],1))\n",
    "for i in range(valid_set1.shape[0]):\n",
    "    valid_dtws[i,0] = dtw(valid_set1[i,0:23], valid_set1[i,23:])**2\n",
    "\n",
    "test_dtws = numpy.zeros((test_set1.shape[0],1))\n",
    "for i in range(test_set1.shape[0]):\n",
    "    test_dtws[i,0] = dtw(test_set1[i,0:23], test_set1[i,23:])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the neural network model\n",
    "# start the tensorflow interaction interface\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hiddens = [100,10]\n",
    "learning_rate = [1e-2]\n",
    "training_epochs = 10000\n",
    "\n",
    "# create two variable placehold, x for the training features, \n",
    "# y for the labels(in this model it is the dtw distance between two time series)\n",
    "x = tf.placeholder(tf.float32, shape=[None, train_set1.shape[1]])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "nn1 = NeuNet(\n",
    "    None,\n",
    "    None,\n",
    "    input = x,\n",
    "    activation = tf.nn.sigmoid,\n",
    "    n_visible = train_set1.shape[1],\n",
    "    n_hidden =  n_hiddens[0]\n",
    ")\n",
    "\n",
    "nn2 = NeuNet(\n",
    "    None,\n",
    "    None,\n",
    "    input = nn1.output,\n",
    "    activation = tf.nn.sigmoid,\n",
    "    n_visible = n_hiddens[0]*2,\n",
    "    n_hidden = n_hiddens[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the cost and minimize it\n",
    "cost = nn2.cost_function(y)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate[0]).minimize(cost)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, the mean error of the training data 16.554, vilidation data 16.3431\n",
      "step 100, the mean error of the training data 6.68091, vilidation data 6.51247\n",
      "step 200, the mean error of the training data 5.07931, vilidation data 5.24687\n",
      "step 300, the mean error of the training data 4.72187, vilidation data 4.94978\n",
      "step 400, the mean error of the training data 4.53294, vilidation data 4.81859\n",
      "step 500, the mean error of the training data 4.37997, vilidation data 4.72571\n",
      "step 600, the mean error of the training data 4.2631, vilidation data 4.66389\n",
      "step 700, the mean error of the training data 4.11943, vilidation data 4.59007\n",
      "step 800, the mean error of the training data 3.99915, vilidation data 4.52306\n",
      "step 900, the mean error of the training data 3.87423, vilidation data 4.45973\n",
      "step 1000, the mean error of the training data 3.73585, vilidation data 4.38818\n",
      "step 1100, the mean error of the training data 3.606, vilidation data 4.30715\n",
      "step 1200, the mean error of the training data 3.49004, vilidation data 4.26253\n",
      "step 1300, the mean error of the training data 3.38788, vilidation data 4.23318\n",
      "step 1400, the mean error of the training data 3.28646, vilidation data 4.18449\n",
      "step 1500, the mean error of the training data 3.20377, vilidation data 4.14419\n",
      "step 1600, the mean error of the training data 3.1267, vilidation data 4.11453\n",
      "step 1700, the mean error of the training data 3.0588, vilidation data 4.08655\n",
      "step 1800, the mean error of the training data 2.98828, vilidation data 4.04716\n",
      "step 1900, the mean error of the training data 2.91538, vilidation data 4.01085\n",
      "step 2000, the mean error of the training data 2.85089, vilidation data 3.98153\n",
      "step 2100, the mean error of the training data 2.79475, vilidation data 3.96265\n",
      "step 2200, the mean error of the training data 2.73561, vilidation data 3.95581\n",
      "step 2300, the mean error of the training data 2.69484, vilidation data 3.947\n",
      "step 2400, the mean error of the training data 2.65462, vilidation data 3.94418\n",
      "step 2500, the mean error of the training data 2.61755, vilidation data 3.93543\n",
      "step 2600, the mean error of the training data 2.57896, vilidation data 3.92433\n",
      "step 2700, the mean error of the training data 2.54157, vilidation data 3.91872\n",
      "step 2800, the mean error of the training data 2.51443, vilidation data 3.91851\n",
      "step 2900, the mean error of the training data 2.48438, vilidation data 3.91031\n",
      "step 3000, the mean error of the training data 2.45462, vilidation data 3.90533\n",
      "step 3100, the mean error of the training data 2.42958, vilidation data 3.90348\n",
      "step 3200, the mean error of the training data 2.40681, vilidation data 3.90343\n",
      "step 3300, the mean error of the training data 2.38005, vilidation data 3.90506\n",
      "step 3400, the mean error of the training data 2.35501, vilidation data 3.90146\n",
      "step 3500, the mean error of the training data 2.33353, vilidation data 3.90404\n",
      "step 3600, the mean error of the training data 2.30896, vilidation data 3.90797\n",
      "step 3700, the mean error of the training data 2.28556, vilidation data 3.91064\n",
      "step 3800, the mean error of the training data 2.26282, vilidation data 3.90952\n",
      "step 3900, the mean error of the training data 2.24464, vilidation data 3.90822\n",
      "step 4000, the mean error of the training data 2.22879, vilidation data 3.90975\n",
      "step 4100, the mean error of the training data 2.21283, vilidation data 3.91734\n",
      "step 4200, the mean error of the training data 2.19452, vilidation data 3.92149\n",
      "step 4300, the mean error of the training data 2.18028, vilidation data 3.92593\n",
      "step 4400, the mean error of the training data 2.1614, vilidation data 3.92525\n",
      "step 4500, the mean error of the training data 2.14202, vilidation data 3.93506\n",
      "step 4600, the mean error of the training data 2.12706, vilidation data 3.93733\n",
      "step 4700, the mean error of the training data 2.11372, vilidation data 3.94362\n",
      "step 4800, the mean error of the training data 2.08891, vilidation data 3.94622\n",
      "step 4900, the mean error of the training data 2.07497, vilidation data 3.94187\n",
      "step 5000, the mean error of the training data 2.06379, vilidation data 3.942\n",
      "step 5100, the mean error of the training data 2.0533, vilidation data 3.94423\n",
      "step 5200, the mean error of the training data 2.04029, vilidation data 3.94523\n",
      "step 5300, the mean error of the training data 2.03175, vilidation data 3.94241\n",
      "step 5400, the mean error of the training data 2.02131, vilidation data 3.94612\n",
      "step 5500, the mean error of the training data 2.01398, vilidation data 3.9483\n",
      "step 5600, the mean error of the training data 2.00602, vilidation data 3.95034\n",
      "step 5700, the mean error of the training data 1.99781, vilidation data 3.94958\n",
      "step 5800, the mean error of the training data 1.98773, vilidation data 3.95026\n",
      "step 5900, the mean error of the training data 1.98222, vilidation data 3.94731\n",
      "step 6000, the mean error of the training data 1.97512, vilidation data 3.94984\n",
      "step 6100, the mean error of the training data 1.96843, vilidation data 3.95329\n",
      "step 6200, the mean error of the training data 1.96251, vilidation data 3.95424\n",
      "step 6300, the mean error of the training data 1.95715, vilidation data 3.95705\n",
      "step 6400, the mean error of the training data 1.95127, vilidation data 3.96362\n",
      "step 6500, the mean error of the training data 1.94485, vilidation data 3.96536\n",
      "step 6600, the mean error of the training data 1.93938, vilidation data 3.96904\n",
      "step 6700, the mean error of the training data 1.93488, vilidation data 3.96766\n",
      "step 6800, the mean error of the training data 1.92944, vilidation data 3.971\n",
      "step 6900, the mean error of the training data 1.926, vilidation data 3.97669\n",
      "step 7000, the mean error of the training data 1.91658, vilidation data 3.97739\n",
      "step 7100, the mean error of the training data 1.91205, vilidation data 3.9815\n",
      "step 7200, the mean error of the training data 1.90815, vilidation data 3.98368\n",
      "step 7300, the mean error of the training data 1.90658, vilidation data 3.98164\n",
      "step 7400, the mean error of the training data 1.90063, vilidation data 3.98608\n",
      "step 7500, the mean error of the training data 1.89687, vilidation data 3.98786\n",
      "step 7600, the mean error of the training data 1.89119, vilidation data 3.99039\n",
      "step 7700, the mean error of the training data 1.88554, vilidation data 3.99028\n",
      "step 7800, the mean error of the training data 1.88111, vilidation data 3.99078\n",
      "step 7900, the mean error of the training data 1.87601, vilidation data 3.99286\n",
      "step 8000, the mean error of the training data 1.87234, vilidation data 3.99173\n",
      "step 8100, the mean error of the training data 1.86773, vilidation data 3.99671\n",
      "step 8200, the mean error of the training data 1.86452, vilidation data 3.99608\n",
      "step 8300, the mean error of the training data 1.85994, vilidation data 3.99485\n",
      "step 8400, the mean error of the training data 1.85535, vilidation data 3.99241\n",
      "step 8500, the mean error of the training data 1.85423, vilidation data 3.99419\n",
      "step 8600, the mean error of the training data 1.8499, vilidation data 3.99381\n",
      "step 8700, the mean error of the training data 1.84726, vilidation data 3.9942\n",
      "step 8800, the mean error of the training data 1.84387, vilidation data 3.99781\n",
      "step 8900, the mean error of the training data 1.8397, vilidation data 3.99203\n",
      "step 9000, the mean error of the training data 1.83734, vilidation data 3.99098\n",
      "step 9100, the mean error of the training data 1.83137, vilidation data 3.99152\n",
      "step 9200, the mean error of the training data 1.82929, vilidation data 3.98935\n",
      "step 9300, the mean error of the training data 1.82698, vilidation data 3.98951\n",
      "step 9400, the mean error of the training data 1.8253, vilidation data 3.9873\n",
      "step 9500, the mean error of the training data 1.8234, vilidation data 3.98816\n",
      "step 9600, the mean error of the training data 1.82152, vilidation data 3.98818\n",
      "step 9700, the mean error of the training data 1.81971, vilidation data 3.98714\n",
      "step 9800, the mean error of the training data 1.8195, vilidation data 3.98395\n",
      "step 9900, the mean error of the training data 1.81674, vilidation data 3.97924\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "train_error = []\n",
    "valid_error = []\n",
    "best_valid_error = numpy.inf\n",
    "for i in range(training_epochs):\n",
    "    sess.run([train_step], feed_dict={x:train_set1, y:train_dtws})\n",
    "    if i%100 == 0:\n",
    "        train_err = sess.run([cost],feed_dict={x:train_set1, y:train_dtws})\n",
    "        train_error.append(train_err)\n",
    "        valid_err = sess.run([cost],feed_dict={x:valid_set1, y:valid_dtws})\n",
    "        valid_error.append(valid_err)\n",
    "        print(\"step %d, the mean error of the training data %g, vilidation data %g\"%(i, train_error[-1][0], valid_error[-1][0]))\n",
    "        #print h_conv1_flat.eval(feed_dict={x:test_set1})\n",
    "        if valid_error[-1][0] < best_valid_error * 0.995:\n",
    "            W_1 = sess.run(nn1.W)\n",
    "            b_1 = sess.run(nn1.b)\n",
    "            W_2 = sess.run(nn2.W)\n",
    "            b_2 = sess.run(nn2.b)\n",
    "            best_valid_error = valid_error[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHH1JREFUeJzt3XuYG+Vh7/HvSFppLzZrG9sYbOwNdtYOHGMwxiaJHYRj\nEkiBcM7znBwoaQlp2tM0p1BIQ6B5nmSfnCctlzYJpE04bR9zyQlOm+s5zgkEmyAwBWwciMPF+LJ2\nYNfGiy9rfNsd3eb88WpWI6HdlbUaSd75fZ5nnh3N7X01K/3e0TsjDYiIiIiIiIiIiIiIiIiIiIiI\niIiISJWsBvqAVzzTlgKbgJeBF4GL61AvERGpohXAhRSGfQL4eG78SuCpGtdJREROUmiU+RuA/qJp\nbwPtufFJwJ5qV0pERGqvg8Ij+zlAD/AW0AucXYc6iYhIlXVQGPbrgf+cG/+vwLpaV0hERE6OVcYy\nHcBaYGHu8RHgNM/6h8l36wyZO3eu093dXYUqiogESjcwr9obHa3PvpSdwKW58ZXA9lILdXd34ziO\nBsfha1/7Wt3r0CiD9oX2hfbFyAMwt7I4H1lklPlrMME+FdNP/1Xgz4B/AmLAQO6xiIg0sNHC/vph\npi+rdkVERMQ/lXTjyEmKx+P1rkLD0L7I077I077wXzknaCvl5PqfRESkTJZlgQ/ZrCN7EZEAUNiL\niASAwl5EJAAU9iIiAaCwFxEJAIW9iEgAKOxFRAJAYS8iEgAKexGRAFDYi4gEgMJeRCQAFPYiIgGg\nsBcRCQCFvYhIAIwW9quBPgpvOA7wl8BW4FXgbh/qJSIiVTTanaoeBL4DPOKZdhlwDXA+kAKm+VM1\nERGpltGO7DcA/UXTPg/8HSboAfZXu1IiIlJdlfTZvx/4CPACkACWVLNCIiJSfaN14wy3zmTgEuBi\n4N+Bc0ot2NXVNTQej8d1n0kRkSKJRIJEIuF7OeXc57ADWAsszD1+DLgLeDr3eCewDDhYtJ7uQSsi\ncpIa6R60PwdW5sY7gSjvDXoREWkgo3XjrAEuBU4HeoCvYi7HXI25HDMJ/LGfFRQRkbGr+kcFD8fJ\nZsHyswgRkfGlkbpxypdM+rp5EREpj79hb9u+bl5ERMrja9hnBk74uXkRESmTr2FvDxz1c/MiIlIm\nf8P+hMJeRKQR+Bv2gwp7EZFG4GvYDw4c83PzIiJSJp+P7BX2IiKNwOewP+7n5kVEpEz+hr2tsBcR\naQQ+h72usxcRaQQKexGRAPA37JMKexGRRuBz2A/4uXkRESmTv2GfUtiLiDQChb2ISACMFvargT7M\nXamKfRHIAlOGW9lODVZeMxERqZrRwv5B4IoS088GLgfeHGllO62wFxFpBKOF/Qagv8T0bwK3j7Zx\nO62bl4iINIJK+uw/CfQCvxttQTujsBcRaQSRk1y+FfgbTBeOa9gb4z7x9G5SXV0AxONx4vH4SRYn\nIjK+JRIJEomE7+WUcwfzDmAtsDA3rAfcb0vNAvYAS4F3itZzbv/i+dz991uqU1MRkQCwLAvKy+aT\ncrJH9q8AZ3ge7wYuAg6VWtjOpiqsloiIVNNoffZrgOeATqAHuKlovjPSynY2WXnNRESkakY7sr9+\nlPnnjDRTR/YiIo3B32/QOgp7EZFG4HPYp/3cvIiIlMnfsEdhLyLSCHwO+4yfmxcRkTIp7EVEAsDf\nsLcU9iIijUBhLyISAD6HfdbPzYuISJn8DfvQiF+wFRGRGvE37MMKexGRRqAjexGRAPA37CNARidp\nRUTqzeduHMDW3apEROrN17BPRsAZ1E3HRUTqzdewb8pAcuCon0WIiEgZfA37WMbCPqGwFxGpt3LC\nfjXQh7kloeteYCuwBfgp0F5qxVjWYlBH9iIidVdO2D8IXFE07QngPGARsB24s9SKMSeEPXBsTBUU\nEZGxKyfsNwD9RdPWAe5vIWwEZpVaMZYNYQ8q7EVE6q0affafBX5ZakbMCSvsRUQawGg3HB/NV4Ak\n8Gipmf3/keb+3n/jzOe7icfjxOPxMRYnIjK+JBIJEomE7+VYZS7XAawFFnqmfQb4U+CjQKmL6Z2l\nX2zn20u/ygc/ddtY6igiEhiWZUH52Vy2So/srwC+BFxK6aAHIEYYO3miwiJERKRayumzXwM8B8wH\nejB99N8BJmBO1L4MfLfUijErgm0r7EVE6q2cI/vrS0xbXc7GY1aTjuxFRBqAv9+gtZqwUwN+FiEi\nImXwN+xDTdgp/RCaiEi91SDsdWQvIlJvPod9VEf2IiINwN+wD0ex07p5iYhIvfkc9jHsjMJeRKTe\nfA375kizjuxFRBqAv0f2kRh2VmEvIlJvPod9M3Ym6WcRIiJSBn/DvqkZO5vyswgRESmDz2HforAX\nEWkA/oZ9tAXbUdiLiNSbwl5EJAB8DvtWbCftZxEiIlIG/8Mehb2ISL2NFvargT7gFc+0KZiblmwH\nngAmDbdyrLkNm8xY6ygiImM0Wtg/iLkFodcdmLDvBJ7MPS4pFmvDthT2IiL1NlrYbwD6i6ZdAzyc\nG38YuHa4lWPNbdhWtvLaiYhIVVTSZ38GpmuH3N8zhlsw1jxBR/YiIg1grCdondxQUqxlAnZo2Nki\nIlIj5dxwvFgfMAPYB5wJvDPcgt974H/T90KGrq4u4vE48Xi8slqKiIxTiUSCRCLhezlWGct0AGuB\nhbnH9wAHgbsxJ2cnUfokrbP3nW4uvGcu++7V0b2ISDksy4LysvmkjNaNswZ4DpgP9AA3AXcBl2Mu\nvVyZe1xSrPU07DDgKOxFROpptG6c64eZvqqcjceaWrAjQCoF0ehJVUxERKrH/5uXhMEZ1E3HRUTq\nydewj4TMB4f0wHE/ixERkVH4GvYAsQzYA0f9LkZEREbgf9hnQwp7EZE6q0HYW9gDx/wuRkRERuB/\n2Dshhb2ISJ3VphtnUGEvIlJP/oc9YWxbV+OIiNRTDbpxwtiDCnsRkXqqwZF9BNs+4XcxIiIygtp0\n4yQV9iIi9eR/2FtNCnsRkTqrQdhHsJMDfhcjIiIj8D/sQ00KexGROqtN2KcU9iIi9VSDsI9ip/UT\nxyIi9TSWsL8TeA14BXgUiJVayIS9PYZiRERkrCoN+w7gT4HFmHvThoHrSi0YC8d0ZC8iUmeVhv0R\nIAW0Ym5t2ArsKbVgcySmI3sRkTqrNOwPAf8AvAXsBQ4D60stGIvEsDPJCosREZFqqDTs5wJ/henO\nOQuYANxQasFYpBk7qyN7EZF6ilS43hLgOeBg7vFPgQ8BP/Au1NXVxQsv7KD/3T4SiQTxeLziioqI\njEeJRIJEIuF7OVaF6y3CBPvFwCDwELAJ+CfPMo7jOHzvu59ly/ZneODbO8dUURGRILAsCyrP5mFV\n2o2zBXgE2Az8Ljftn0stGIs2YzupCosREZFqqLQbB+Ce3DCiWFOrwl5EpM78/wZttAXbSftdjIiI\njMD/sI+1KuxFROqsBmHfhk3G72JERGQEtTmytxT2IiL15H/YN0/Qkb2ISJ3VIOzbsEMKexGReqrN\nkb2V9bsYEREZgf9h3zIBO+T4XYyIiIygBmE/UWEvIlJn/od960TssMJeRKSeatNnHway6rcXEakX\n/8M+0sxgBBjUrQlFROrF97CPhqOkw5DtecvvokREZBi+h71lWUSzIexXf+t3USIiMgzfwx5gJhN5\n8/Xna1GUiIiUUJOwXzpxARt7N9aiKBERKWEsYT8J+DGwFXgduGS4BZfN+TCbBnaMoSgRERmLsYT9\nfcAvgQ8A52NCv6Sliz7BxtZ+SOt37UVE6qHSsG8HVgCrc4/TwLvDLbz4fR/i9WkwuO21CosTEZGx\nqDTs3wfsBx4EXgL+BWgdbuGWphYWJE/j5Zf+X4XFiYjIWFR6w/EIsBj4H8CLwLeBO4Cvehfq6uoa\nGp/19jQ2tW7ggxUWKCIyHiUSCRKJhO/lWBWuNwN4HnOED7AcE/ZXeZZxHCf/mzgPfe+/88T2x3j0\nW/pylYjIcCzLgsqzeViVduPsA3qAztzjVcCIHfJL/9PH2RjZV2FxIiIyFmO5GucvgR8AWzBX4/zt\nSAsvWHIFByIpDhzeO4YiRUSkEmMJ+y3AxcAi4L8wwtU4AKGWVpYcbmHT5v8zhiJFRKQSNfkGrWuZ\ndTabtv26lkWKiAi1Dvupi9i4/+VaFikiItQ47Jd2XsamTA/eq3RERMR/NQ37Mxctp83O0t3fXcti\nRUQCr6Zhz/z5LOvJsva1n9W0WBGRoKtt2EejfH1XB3f/x908tuOxmhYtIhJktQ174ANzLuJnEz/H\njT+/Ub9xLyJSIzUPe265hQ/+z4d46MP38skffpKt+4f9ZWQREamS2of9hz8Mt97KJ/76f3Hvyr9j\n+YPLuXP9new/vr/mVRERCYrahz3Al74EU6bwR4++xm/+7DccHjzM/H+cz62P38qr77xalyqJiIxn\nVf9lNQ9nxOvpDx6ExYvhvvvg2mvZc2QP9228jzWvrmFS8ySuO+86/qDzD1g4fSHhUNjHaoqINA6/\nfvWyfmEPsHEjXH013Hwz3HEHRCJknSzP9TzHD1/9IU/ufpK9R/eybOYyls9ezsfmfowlZy0hEqr0\nZ/hFRBrb+Ax7gN5euPFGGBiA738f5s4tmH3gxAGe73mep998mnW71vHWu2+x8n0ruazjMuIdcc6d\ndi4hqz69USIi1TZ+wx4gm4X774dvfANuuw1uvRWam0su+vbRt1m/az2J3yd4+s2nOTx4mBVzVrBi\n9gqWz17OhTMupCncVMWnISJSO+M77F07d8Ltt8NLL8Hdd8OnPgXWyFXsPdLLhjc38Oxbz/Jsz7Ps\n7t/N8tnLWXXOKlads4qF0xe6O09EpOEFI+xdiYQ5ws9k4JZb4A//cNgj/WKHBg7x1O6nWL9rPet2\nrWMwPchVnVdxdefVXNpxKROiEyqrk4hIDTRq2IeBzUAvcHXRvMrD3qwN69aZq3U2b4Y//3PTALS3\nn8QmHLYd3MbabWtZu30tm/duZnrbdM6ddi4Lpi5gdvtsZk6cyazTZrFg6gImt0yuvL4iIlXQqGF/\nG3ARMBG4pmje2MLea/t2uOsu+MUvTDfPF74ALS0nvZlMNsPuw7t5ff/rvHHgDXqP9NJ7pJeeIz28\nceANJjdP5vwzzue8aecxf+p8Ok/vpPP0Tqa1TlNXkIjURCOG/SzgIeAbmNCv7pF9KVu3wle+Ai++\naE7i/smfnNSR/kiyTpbd/bvZ0reFrfu3sv3QdrYf3M6Ogzs4ljzGzNNmMnPizKG/Z008i9nts+k8\nvZN5U+bR2tRalXqISLA1Ytj/CHOT8dOAv6YWYe/avBm++U341a/MZZu33AJz5vhTFjCQGmDP0T30\nvNvD3qN72XN0D3uP7uX3h3/PjkM72NW/i6mtU5k7eS7nTD6Hcyafw5z2Ocw8zXQRzZgwg4nRifp0\nIHXnOA4ZJzN0AyEHh0w2QzqbJp1Nk3EyZJ0sWSeL4ziErBCWZRGyQmSyGVLZlFkum8HBGVrOu563\nLLcM93HWyQ6tB2BhYVkWWSdLOpsmlUmRcTJD23Dnu39DVoiQFSJshbEsq2Cboz1n9zmGrTDRcJSm\ncBORUKSgDG99LfLlFb933ekhK4SFhYMz7E2Z3G2HrBDhUJiwFSYcCtMUMuVHQhHaom1Dl5D7FfaV\nfjvpKuAd4GUgPtxCXV1dQ+PxeJx4fNhFT86SJfDoo9DTYy7ZXLwYrrjCdPEsWlSdMjxamlqYN2Ue\n86bMKzk/k83Qc6SHXf276D7Uza7+Xfyq+1dD3UT7ju3DzthMaZnC5ObJNEeaiYajxCIxmiPNQ0M0\nHCVshQlZISKhCLGwmd/S1EJLpIW2aButTa20RFqGXqxNoaaC7xk0hZuG1otFYjSFmoZe1O62i1+8\njuMUvHG9L0r3jZjJZobekMMFAxS+sN03p3f76WwaO21jZ2zstF2wvXQ2PRQm3jBwcBhMDzKQGuBE\n6sTQ/KGAyKZIZpIkM0lTT0x93f3oPveh55t7Y7r1d+vnvslD5J9/xsmYMHTSQ/vAO3iDy62vW4bL\nDadwKFywrhuybhluvYrrl86mSWaS2GmbVDY17DayTrZg32eymYKQS2aSpLPpoYByXwPefRQJRYZe\nI+7zcPdROBQuWNYtx8IqeL1Ynpxyy3CnFb/+vK8bN/zCoXBBgHr/Fu9/73PxluvgFDweqnfuf5DM\nJEllzGvN+xy99S0uz/u/dRuZUg2Tl7f+7vLF/5fBHYN8bsrnOL319FLxUjWVth5/C/wRkAaaMUf3\nPwH+2LOMf0f2xd59Fx54wJzMveAC6OqCpUtrU3aZ7LRN/2A//QP92Bl76M1rZ2wGUgMMpgexM3bB\nG9jO2EMhN5Ae4HjyOCdSJziRPkEqkyKVTZHKpAqOnNLZNIPpwaHtuS9ob0hkspn31M/7xvW+KB2c\ngkbC23C4b3B33eHemN4AchuxWCRGLBwr2J477jZg3qOllkjLUKMXDUcLjqxikZhp/HJh4U53cAoa\nEu+b37t+0RFVQePmNhhu0HmP5rzB5YaD+3/wTnO3l3EyQ8Hvrutu2xuUxUew4VCYWDg21MB797k3\npL3/g6yTHZruHkW6Bwf6hNnYGrEbx3Upte7GGY5tw+rV5stZixfD179uwl9E5BThV9hX63cGGuMO\n4rEYfP7z5stZH/0oXHklXHutOaErIhJgjfmlqmoZGIB//Ve4915YsAC+/GVYuXLUb+WKiNRLI3fj\nDKf+Ye9KJs2PrH3rW+bxzTfDDTdAW1t96yUiUkRhXw2OA7/+tbmC59lnzW/v3HQTXHyxjvZFpCEo\n7KvtrbfgkUfgoYfM7+585jPw6U/DjBn1rpmIBJjC3i+OAxs2mND/2c/MPXJvuMGc3J00qd61E5GA\nUdjXwrFj8JOfwI9+BM88Y67Vv+Ya+MQnYF7pL1SJiFSTwr7Wjh+HJ56AtWvh8cfNydwrr4RVq2DF\nCpisX8gUkepT2NeT48CWLfDYY/DUU/D889DZCZddZi7l/MhHYIJ+J19Exk5h30iSSdi0yQT/k0+a\nH2a74AK4/HIzLF0KEd0UXUROnsK+kZ04YU7yrl9vbrjy5pvmRO+KFWZYsgSi0XrXUkROAQr7U0lf\nnwl/d3jjDVi40FzPf/HFcMklphtI1/aLSBGF/ans2DF4+WXzGz2bNsHGjeaXOpctMz/J3NkJ8+eb\n4fTT1QiIBJjCfrzZtw9eeAFeew22bcsPoZAJ/85OmDvXDPPmmUENgci4p7APAseBAwfMPXe3bYPu\nbti1y/yK586dZv7732+C320E5s6FWbPgzDPNN4FF5JSmsA86x4FDh2DHDjN0d+eHvXvh7bfN5Z8z\nZ5rwnzULzj47P+42CO3t+nQg0sAU9jIyx4GDB2HPHnO7RnfYswd6e82wb5+5wcuMGTB9uhmmTTN/\nzzrLNAbuMGOGvjsgUgcKe6mOEydM6L/zDuzfb/729ZlPBm+/bT4luI9DIdMQTJ1qGoWpU+GMM0xD\ncMYZZnAbjalT9d0CkSpoxLA/G3gEmI65U9U/A/d75ivsT2WOA0ePmgbhwAHzd/9+0xD09eUbDHc4\ndAhOOy3/aWHyZJgyxfxtby8cJk3K/3WHcHj0OokEQCOG/Yzc8FtgAvAb4Fpga26+wj5IMhno789/\nYujvN8OhQ+Yy08OHzd/iob8fjhwxXUbt7TBxomk02ttNY1FqmDQpv8zEiWbQpwoZJxox7Iv9HPgO\n8GTuscJeypPNmuA/csQMR4+axsFtLA4eNH/dce+yR46Y7zE0NZnQnz4939U0ebJpFCZONI1JW1t+\naG3ND21tZv6ECeaxTmBLHTV62HcATwPnAcdy0xT2UhuOY+43fORI/hxEX1/+U8PRo2Y4ccIMx48X\njh8/bhqMY8dgcNDcuL652Qxug9DSYoZYLD80Nxcu6467y7rrNTXlh2i0cBvRaH6aOz8azS+vhidw\n/Ar7anz2nQD8GLiFfNAD0NXVNTQej8eJx+NVKE6kiGXlQ3msdxrLZs0VS4ODpgEZGMg3CoODZp47\nv3h8cNAMR44UrptK5YdkMr+ebZvH7jR3GXc8nTbnMryNgHfwNiDueCRS+Nc7PRzOT/c2SsXreZe1\nLHOiPhTKLxONvne54erjLd9dPxSqzv99nEgkEiQSCd/LGWvr0QT8AngM+HbRPB3Zi4yF45jATybf\n2xC4jYQ77s5Pp83gbWDc6ZlMfp7bGA0O5uenUvll3MFxzOBdN5ks3J53urdRc5crrlMoVNgAeRsh\nbyPiTvc2FN7H3sbIsgqXd9cPh818d17xNt357uBtSCORwnnDDd4GsdQ0bx1d7jS3/IsuMg0vjdmN\nYwEPAweBW0vMV9iLSCG34fA2QMWNRvF07/LeBs3bGGWzZvDOy2TMtOJtlWrQ3OW8DZW7vjvPu1zx\neKl52WzhuJd3u9ksrFkDs2cDjRn2y4FngN9hLr0EuBN4PDeusBcROUmNGPajUdiLiJwkv8JeZ0pE\nRAJAYS8iEgAKexGRAFDYi4gEgMJeRCQAFPYiIgGgsBcRCQCFvYhIACjsRUQCQGEvIhIACnsRkQBQ\n2IuIBIDCXkQkABT2IiIBoLAXEQmAsYT9FcAbwA7gy9WpjoiI+KHSsA8D/4gJ/HOB64EPVKtS400t\nbiZ8qtC+yNO+yNO+8F+lYb8U2An8HkgBPwQ+WaU6jTt6IedpX+RpX+RpX/iv0rCfCfR4HvfmpomI\nSAOqNOx1c1kRkVNIpTe1vQTowvTZA9wJZIG7PcvsBOZWXDMRkWDqBubVuxKuCKZCHUAU+C06QSsi\nMi5dCWzDHMHfWee6iIiIiIiIH8b7F67OBp4CXgNeBW7OTZ8CrAO2A08Akzzr3InZH28AH/NMvwh4\nJTfvPl9r7a8w8DKwNvc4qPtiEvBjYCvwOrCM4O6LOzHvkVeAR4EYwdkXq4E+TL1d1XzuMeDfctNf\nAOZUt/rlCWO6djqAJsZnf/4M4ILc+ARMd9YHgHuA23PTvwzclRs/F7MfmjD7ZSf5k+ObMN9bAPgl\n+ZPep5rbgB8A/zf3OKj74mHgs7nxCNBOMPdFB7ALE0pggulGgrMvVgAXUhj21XzufwF8Nzf+3zDf\ndaq5DwKPex7fkRvGs58DqzCt8hm5aTNyj8G02t5POI9jrmg6E3ME6LoOeMDXmvpjFrAeuIz8kX0Q\n90U7JuCKBXFfTMEcBE3GNHprgcsJ1r7ooDDsq/ncH8d8agSzf/ePVhk/fggtaF+46sC04Bsx/8i+\n3PQ+8v/YszD7weXuk+Lpezg199W3gC9hLr91BXFfvA/zpnsQeAn4F6CNYO6LQ8A/AG8Be4HDmC6M\nIO4LVzWfuzdn08C7mAZ2WH6EfZC+cDUB+AlwC3C0aJ5DMPbFVcA7mP764b63EZR9EQEWYz5eLwaO\n895PtUHZF3OBv8IcDJ2Fea98umiZoOyLUmr+3P0I+z2YE5iusylsncaLJkzQfx/TjQOmtZ6RGz8T\nE4Lw3n0yC7NP9uTGvdP3+FRfv3wIuAbYDawBVmL2SRD3RW9ueDH3+MeY0N9H8PbFEuA54CDmyPOn\nmC7eIO4LVzXeE72edWbnxt1zQ4eqX+WRBeELVxbwCKb7wuse8n1vd/DeEzBRzEf9bvJHwRsxfW8W\np87Jp+FcSr7PPqj74hmgMzfehdkPQdwXizBXqrVgnsPDwBcI1r7o4L0naKv13P8C+F5u/DrqdIIW\nxv8XrpZj+qd/i+m+eBnzT5iCOVFZ6tKqv8HsjzeAj3umu5dW7QTu97viPruU/NU4Qd0XizBH9lsw\nR7PtBHdf3E7+0suHMZ+Gg7Iv1mDOVSQxfes3Ud3nHgP+nfyllx0+PAcRERERERERERERERERERER\nERERERERERERkVPX/wekJRmitQXZDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56941d0950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error and validation error\n",
    "Xaxis = [x * 100 for x in range(training_epochs/100)]\n",
    "plt.plot(Xaxis, train_error, 'r',label='train error')\n",
    "plt.plot(Xaxis, valid_error, 'g',label='validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the test dataset\n",
    "test_set_rescale = test_set/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape the W matrix to [23,10]\n",
    "W_1_prime = numpy.reshape(W_1,[23, n_hiddens[0]])\n",
    "hidden_1 = sess.run(tf.nn.sigmoid(numpy.matmul(test_set,W_1_prime)+b_1))\n",
    "W_2_prime = numpy.reshape(W_2,[n_hiddens[0], n_hiddens[1]])\n",
    "hidden_2 = tf.nn.sigmoid(numpy.matmul(hidden_1,W_2_prime)+b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = sess.run(hidden_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16397669e-07,   1.31777271e-03,   1.99405509e-01, ...,\n",
       "          9.99999978e-01,   1.00000000e+00,   7.76016295e-06],\n",
       "       [  7.47261553e-04,   3.82610180e-04,   9.99999977e-01, ...,\n",
       "          1.00000000e+00,   9.99999996e-01,   7.78138190e-01],\n",
       "       [  1.87070623e-09,   3.24778967e-14,   1.00000000e+00, ...,\n",
       "          1.00000000e+00,   1.00000000e+00,   9.64337943e-01],\n",
       "       ..., \n",
       "       [  9.96187019e-01,   9.52355797e-01,   9.99999980e-01, ...,\n",
       "          1.00000000e+00,   9.99991521e-01,   9.99999586e-01],\n",
       "       [  3.34437230e-03,   8.62445490e-11,   2.65457901e-02, ...,\n",
       "          4.89121225e-08,   1.00000000e+00,   1.85822076e-04],\n",
       "       [  1.97823723e-14,   1.91019661e-12,   1.00000000e+00, ...,\n",
       "          9.10287052e-01,   1.00000000e+00,   6.36816465e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for one time series in test dataset, calculate its distance with 1000 time series in test dataset\n",
    "euclidean_dists = []\n",
    "dtw_dists = []\n",
    "for i in range(1000):\n",
    "    dtw_dists.append((i,dtw(test_set_rescale[i], test_set_rescale[-3])))\n",
    "    euclidean_dists.append((i, euclideanDist(test_features[i], test_features[-3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the result\n",
    "euclidean_dists_sorted = sorted(euclidean_dists, key=(lambda x: x[1]))\n",
    "dtw_dists_sorted = sorted(dtw_dists, key=(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(137, 0.048786173385465925),\n",
       " (979, 0.04947798180306272),\n",
       " (951, 0.050912793021390956),\n",
       " (617, 0.052762406541472545),\n",
       " (947, 0.054013142361485356),\n",
       " (616, 0.0542590919779111),\n",
       " (193, 0.054457731611809904),\n",
       " (998, 0.05474820947396863),\n",
       " (219, 0.05546584005742464),\n",
       " (292, 0.057448826870309735),\n",
       " (327, 0.057695249458845986),\n",
       " (459, 0.05917192932128024),\n",
       " (749, 0.0644046344740277),\n",
       " (143, 0.06794457990333636),\n",
       " (456, 0.06821947028059804),\n",
       " (231, 0.0682349685141227),\n",
       " (743, 0.07601540057423666),\n",
       " (689, 0.0771653073653056),\n",
       " (74, 0.07912596342578367),\n",
       " (417, 0.0797564657142975)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_dists_sorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(908, 0.992156862745098),\n",
       " (168, 1.0274509803921568),\n",
       " (872, 1.0745098039215686),\n",
       " (846, 1.0901960784313725),\n",
       " (842, 1.164705882352941),\n",
       " (81, 1.1803921568627447),\n",
       " (80, 1.223529411764706),\n",
       " (847, 1.2549019607843137),\n",
       " (584, 1.2705882352941178),\n",
       " (843, 1.3098039215686272),\n",
       " (153, 1.329411764705882),\n",
       " (163, 1.333333333333333),\n",
       " (169, 1.3333333333333333),\n",
       " (299, 1.4196078431372545),\n",
       " (342, 1.4196078431372547),\n",
       " (799, 1.4313725490196079),\n",
       " (553, 1.4901960784313717),\n",
       " (575, 1.490196078431373),\n",
       " (552, 1.494117647058823),\n",
       " (433, 1.4980392156862743)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtw_dists_sorted[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# take out 100 neighbors using both euclidean distance and dtw distance. \n",
    "# See the number of the overlap neighbors between them\n",
    "euclid_set = set()\n",
    "dtw_set = set()\n",
    "for i in range(100):\n",
    "    euclid_set.add(euclidean_dists_sorted[i][0])\n",
    "    dtw_set.add(dtw_dists_sorted[i][0])\n",
    "count = 0\n",
    "for x in euclid_set:\n",
    "    if x in dtw_set:\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
