{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contained the feature-based similarity search method using two-layer neural network\n",
    "\n",
    "Created by Zexi Chen(zchen22)\n",
    "Date: Oct 2, 2016\n",
    "\"\"\"\n",
    "\n",
    "import numpy\n",
    "import six.moves.cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from Feature_based_similarity_search import loadData, dtw, euclideanDist, NeuNet\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data and rescale to 0-1\n",
    "train_set = loadData('../theano/data/samples1')\n",
    "valid_set = loadData('../theano/data/samples2')\n",
    "test_set = loadData('../theano/data/samples3')\n",
    "\n",
    "# reshape the array, concatenate two time series as one training instance\n",
    "train_set_reshape = numpy.reshape(train_set, (train_set.shape[0]/2, train_set.shape[1]*2))\n",
    "valid_set_reshape = numpy.reshape(valid_set, (valid_set.shape[0]/2, valid_set.shape[1]*2))\n",
    "test_set_reshape = numpy.reshape(test_set, (test_set.shape[0]/2, test_set.shape[1]*2))\n",
    "\n",
    "# re-scale input data\n",
    "train_set1 = train_set_reshape/255.0\n",
    "valid_set1 = valid_set_reshape/255.0\n",
    "test_set1 = test_set_reshape/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTrainData(train_set, numInstance):\n",
    "    sampleIndices = numpy.random.choice(train_set.shape[0], 2 * numInstance)\n",
    "    train_set_samples = train_set[sampleIndices]\n",
    "    train_set_reshape = numpy.reshape(train_set_samples, (train_set_samples.shape[0]/2, train_set_samples.shape[1]*2))\n",
    "    train_set1 = train_set_reshape/255.0\n",
    "    train_dtws = numpy.zeros((train_set1.shape[0],1))\n",
    "    for i in range(train_set1.shape[0]):\n",
    "        train_dtws[i,0] = dtw(train_set1[i,0:23], train_set1[i,23:])**2\n",
    "    \n",
    "    return train_set1, train_dtws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the squared dtw distance between the two time series in each row of the training data validation data and test data \n",
    "# the dtw is used in the cost function as the target value to minimize.\n",
    "train_dtws = numpy.zeros((train_set1.shape[0],1))\n",
    "for i in range(train_set1.shape[0]):\n",
    "    train_dtws[i,0] = dtw(train_set1[i,0:23], train_set1[i,23:])**2\n",
    "\n",
    "valid_dtws = numpy.zeros((valid_set1.shape[0],1))\n",
    "for i in range(valid_set1.shape[0]):\n",
    "    valid_dtws[i,0] = dtw(valid_set1[i,0:23], valid_set1[i,23:])**2\n",
    "\n",
    "test_dtws = numpy.zeros((test_set1.shape[0],1))\n",
    "for i in range(test_set1.shape[0]):\n",
    "    test_dtws[i,0] = dtw(test_set1[i,0:23], test_set1[i,23:])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the neural network model\n",
    "# start the tensorflow interaction interface\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hiddens = [100,50]\n",
    "learning_rate = [1e-2]\n",
    "training_epochs = 10000\n",
    "\n",
    "# create two variable placehold, x for the training features, \n",
    "# y for the labels(in this model it is the dtw distance between two time series)\n",
    "x = tf.placeholder(tf.float32, shape=[None, train_set.shape[1]*2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "nn1 = NeuNet(\n",
    "    None,\n",
    "    None,\n",
    "    input = x,\n",
    "    activation = tf.nn.sigmoid,\n",
    "    n_visible = train_set.shape[1]*2,\n",
    "    n_hidden =  n_hiddens[0]\n",
    ")\n",
    "\n",
    "nn2 = NeuNet(\n",
    "    None,\n",
    "    None,\n",
    "    input = nn1.output,\n",
    "    activation = tf.nn.sigmoid,\n",
    "    n_visible = n_hiddens[0]*2,\n",
    "    n_hidden = n_hiddens[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the cost and minimize it\n",
    "cost = nn2.cost_function(y)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate[0]).minimize(cost)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, the mean error of the training data 1538.73, vilidation data 16.3377\n",
      "step 100, the mean error of the training data 724.258, vilidation data 13.014\n",
      "step 200, the mean error of the training data 598.215, vilidation data 11.7895\n",
      "step 300, the mean error of the training data 646.972, vilidation data 11.8228\n",
      "step 400, the mean error of the training data 536.198, vilidation data 10.7018\n",
      "step 500, the mean error of the training data 712.139, vilidation data 11.1575\n",
      "step 600, the mean error of the training data 660.817, vilidation data 10.9014\n",
      "step 700, the mean error of the training data 563.082, vilidation data 10.984\n",
      "step 800, the mean error of the training data 596.907, vilidation data 11.0111\n",
      "step 900, the mean error of the training data 702.305, vilidation data 11.0759\n",
      "step 1000, the mean error of the training data 761.268, vilidation data 10.9947\n",
      "step 1100, the mean error of the training data 636.86, vilidation data 11.2117\n",
      "step 1200, the mean error of the training data 599.924, vilidation data 10.6087\n",
      "step 1300, the mean error of the training data 593.957, vilidation data 10.4754\n",
      "step 1400, the mean error of the training data 793.672, vilidation data 10.7931\n",
      "step 1500, the mean error of the training data 538.35, vilidation data 11.302\n",
      "step 1600, the mean error of the training data 666.61, vilidation data 10.8509\n",
      "step 1700, the mean error of the training data 823.684, vilidation data 10.6821\n",
      "step 1800, the mean error of the training data 652.123, vilidation data 10.6923\n",
      "step 1900, the mean error of the training data 610.136, vilidation data 10.9115\n",
      "step 2000, the mean error of the training data 528.118, vilidation data 10.7779\n",
      "step 2100, the mean error of the training data 610.073, vilidation data 11.0282\n",
      "step 2200, the mean error of the training data 541.482, vilidation data 10.6166\n",
      "step 2300, the mean error of the training data 696.008, vilidation data 10.3736\n",
      "step 2400, the mean error of the training data 605.76, vilidation data 10.7359\n",
      "step 2500, the mean error of the training data 692.065, vilidation data 10.7783\n",
      "step 2600, the mean error of the training data 604.025, vilidation data 10.2136\n",
      "step 2700, the mean error of the training data 587.83, vilidation data 11.3511\n",
      "step 2800, the mean error of the training data 615.255, vilidation data 10.6456\n",
      "step 2900, the mean error of the training data 498.663, vilidation data 10.7854\n",
      "step 3000, the mean error of the training data 440.371, vilidation data 10.6101\n",
      "step 3100, the mean error of the training data 712.769, vilidation data 10.4968\n",
      "step 3200, the mean error of the training data 619.835, vilidation data 10.6397\n",
      "step 3300, the mean error of the training data 443.307, vilidation data 10.6112\n",
      "step 3400, the mean error of the training data 503.029, vilidation data 10.2062\n",
      "step 3500, the mean error of the training data 653.609, vilidation data 9.75982\n",
      "step 3600, the mean error of the training data 596.549, vilidation data 9.13027\n",
      "step 3700, the mean error of the training data 370.617, vilidation data 9.32811\n",
      "step 3800, the mean error of the training data 706.606, vilidation data 9.67486\n",
      "step 3900, the mean error of the training data 544.864, vilidation data 9.59556\n",
      "step 4000, the mean error of the training data 652.279, vilidation data 9.68334\n",
      "step 4100, the mean error of the training data 678.422, vilidation data 9.0961\n",
      "step 4200, the mean error of the training data 719.325, vilidation data 9.13456\n",
      "step 4300, the mean error of the training data 631.528, vilidation data 9.12621\n",
      "step 4400, the mean error of the training data 797.328, vilidation data 9.68539\n",
      "step 4500, the mean error of the training data 599.47, vilidation data 9.6254\n",
      "step 4600, the mean error of the training data 667.862, vilidation data 9.36985\n",
      "step 4700, the mean error of the training data 698.269, vilidation data 9.06468\n",
      "step 4800, the mean error of the training data 528.159, vilidation data 9.09686\n",
      "step 4900, the mean error of the training data 627.299, vilidation data 9.67068\n",
      "step 5000, the mean error of the training data 610.205, vilidation data 9.46306\n",
      "step 5100, the mean error of the training data 462.217, vilidation data 8.88496\n",
      "step 5200, the mean error of the training data 825.931, vilidation data 9.98198\n",
      "step 5300, the mean error of the training data 675.124, vilidation data 9.21012\n",
      "step 5400, the mean error of the training data 640.398, vilidation data 9.70983\n",
      "step 5500, the mean error of the training data 748.49, vilidation data 9.131\n",
      "step 5600, the mean error of the training data 650.214, vilidation data 9.68972\n",
      "step 5700, the mean error of the training data 448.99, vilidation data 9.51011\n",
      "step 5800, the mean error of the training data 689.409, vilidation data 8.82356\n",
      "step 5900, the mean error of the training data 633.503, vilidation data 10.1827\n",
      "step 6000, the mean error of the training data 629.215, vilidation data 9.17151\n",
      "step 6100, the mean error of the training data 599.709, vilidation data 9.27096\n",
      "step 6200, the mean error of the training data 641.354, vilidation data 9.87693\n",
      "step 6300, the mean error of the training data 508.211, vilidation data 9.06156\n",
      "step 6400, the mean error of the training data 515.601, vilidation data 10.1886\n",
      "step 6500, the mean error of the training data 550.251, vilidation data 10.3736\n",
      "step 6600, the mean error of the training data 624.831, vilidation data 9.52717\n",
      "step 6700, the mean error of the training data 766.505, vilidation data 9.07958\n",
      "step 6800, the mean error of the training data 549.57, vilidation data 9.64743\n",
      "step 6900, the mean error of the training data 657.35, vilidation data 10.4713\n",
      "step 7000, the mean error of the training data 565.68, vilidation data 9.57028\n",
      "step 7100, the mean error of the training data 582.188, vilidation data 9.5689\n",
      "step 7200, the mean error of the training data 862.723, vilidation data 9.77171\n",
      "step 7300, the mean error of the training data 632.32, vilidation data 9.40477\n",
      "step 7400, the mean error of the training data 744.204, vilidation data 9.77415\n",
      "step 7500, the mean error of the training data 439.758, vilidation data 9.14421\n",
      "step 7600, the mean error of the training data 611.375, vilidation data 9.40779\n",
      "step 7700, the mean error of the training data 662.427, vilidation data 9.52904\n",
      "step 7800, the mean error of the training data 531.29, vilidation data 9.20603\n",
      "step 7900, the mean error of the training data 709.681, vilidation data 8.96335\n",
      "step 8000, the mean error of the training data 596.794, vilidation data 8.73207\n",
      "step 8100, the mean error of the training data 699.958, vilidation data 10.0321\n",
      "step 8200, the mean error of the training data 504.839, vilidation data 9.37696\n",
      "step 8300, the mean error of the training data 690.394, vilidation data 8.94598\n",
      "step 8400, the mean error of the training data 898.374, vilidation data 9.60793\n",
      "step 8500, the mean error of the training data 624.709, vilidation data 9.56779\n",
      "step 8600, the mean error of the training data 733.01, vilidation data 9.00886\n",
      "step 8700, the mean error of the training data 502.88, vilidation data 9.18867\n",
      "step 8800, the mean error of the training data 674.497, vilidation data 9.63531\n",
      "step 8900, the mean error of the training data 848.623, vilidation data 9.30994\n",
      "step 9000, the mean error of the training data 570.315, vilidation data 9.69716\n",
      "step 9100, the mean error of the training data 556.349, vilidation data 9.12105\n",
      "step 9200, the mean error of the training data 788.295, vilidation data 10.2465\n",
      "step 9300, the mean error of the training data 516.582, vilidation data 8.57309\n",
      "step 9400, the mean error of the training data 979.329, vilidation data 9.11305\n",
      "step 9500, the mean error of the training data 587.566, vilidation data 9.07209\n",
      "step 9600, the mean error of the training data 747.277, vilidation data 10.1338\n",
      "step 9700, the mean error of the training data 782.412, vilidation data 9.6762\n",
      "step 9800, the mean error of the training data 674.594, vilidation data 9.67767\n",
      "step 9900, the mean error of the training data 581.64, vilidation data 9.77381\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "train_error = []\n",
    "valid_error = []\n",
    "best_valid_error = numpy.inf\n",
    "numInstance = 1000\n",
    "for i in range(training_epochs):\n",
    "    sample_train_set, sample_train_dtws = generateTrainData(train_set, numInstance)\n",
    "    #sess.run([train_step], feed_dict={x:train_set1, y:train_dtws})\n",
    "    sess.run([train_step], feed_dict={x:sample_train_set, y:sample_train_dtws})\n",
    "    if i%100 == 0:\n",
    "        #train_err = sess.run([cost],feed_dict={x:train_set1, y:train_dtws})\n",
    "        train_err = sess.run([cost],feed_dict={x:sample_train_set, y:sample_train_dtws})\n",
    "        train_error.append(train_err)\n",
    "        valid_err = sess.run([cost],feed_dict={x:valid_set1, y:valid_dtws})\n",
    "        valid_error.append(valid_err)\n",
    "        print(\"step %d, the mean error of the training data %g, vilidation data %g\"%(i, train_error[-1][0], valid_error[-1][0]))\n",
    "        #print h_conv1_flat.eval(feed_dict={x:test_set1})\n",
    "        if valid_error[-1][0] < best_valid_error * 0.995:\n",
    "            W_1 = sess.run(nn1.W)\n",
    "            b_1 = sess.run(nn1.b)\n",
    "            W_2 = sess.run(nn2.W)\n",
    "            b_2 = sess.run(nn2.b)\n",
    "            best_valid_error = valid_error[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX/ax8M6pa1PJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0579ac1cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error and validation error\n",
    "Xaxis = [x * 100 for x in range(training_epochs/100)]\n",
    "plt.plot(Xaxis, train_error, 'r',label='train error')\n",
    "plt.plot(Xaxis, valid_error, 'g',label='validation error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the test dataset\n",
    "test_set_rescale = test_set/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape the W matrix to [23,10]\n",
    "W_1_prime = numpy.reshape(W_1,[23, n_hiddens[0]])\n",
    "hidden_1 = sess.run(tf.nn.sigmoid(numpy.matmul(test_set,W_1_prime)+b_1))\n",
    "W_2_prime = numpy.reshape(W_2,[n_hiddens[0], n_hiddens[1]])\n",
    "hidden_2 = tf.nn.sigmoid(numpy.matmul(hidden_1,W_2_prime)+b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = sess.run(hidden_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.31389595e-04,   3.00213336e-01,   3.92834803e-08, ...,\n",
       "          5.68614611e-06,   8.96090016e-08,   9.99999995e-01],\n",
       "       [  3.68367495e-03,   4.81400907e-02,   1.47490099e-07, ...,\n",
       "          2.79448862e-04,   7.43867452e-10,   9.99999998e-01],\n",
       "       [  2.15633719e-02,   6.91853551e-01,   7.66902298e-08, ...,\n",
       "          3.49492491e-04,   2.68738344e-07,   9.99999996e-01],\n",
       "       ..., \n",
       "       [  1.03478008e-07,   6.96087231e-03,   3.79815705e-07, ...,\n",
       "          7.08011699e-05,   4.43131962e-08,   1.00000000e+00],\n",
       "       [  2.26550830e-06,   9.88732801e-01,   1.12894150e-12, ...,\n",
       "          4.50474864e-01,   9.47424102e-02,   9.99999959e-01],\n",
       "       [  4.47279438e-03,   9.21517615e-01,   1.66548059e-08, ...,\n",
       "          9.99780025e-01,   8.29811371e-01,   9.99999995e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate function\n",
    "def evaluateFunc(num, ori_set, new_set, nn, nNeighbor=100):\n",
    "    '''\n",
    "        num: the number of sample time series we use to evaluate the performance of our neural network model\n",
    "        ori_set: it is the original dataset\n",
    "        new_set: this is the dataset generated after we perform our neural network model on our original dataset\n",
    "        nn: the number of time series we use to calculate the dtw and euclidean distance\n",
    "        nNeighbor: the number of neighbors we extracted to compare the overlap between dtw and euclidean distances\n",
    "    '''\n",
    "    result = []\n",
    "    for n in range(num):\n",
    "        euclidean_dists = []\n",
    "        dtw_dists = []\n",
    "        for i in range(nn):\n",
    "            dtw_dists.append((i,dtw(ori_set[i], ori_set[nn + n])))\n",
    "            euclidean_dists.append((i, euclideanDist(new_set[i], new_set[nn + n])))\n",
    "        euclidean_dists_sorted = sorted(euclidean_dists, key=(lambda x: x[1]))\n",
    "        dtw_dists_sorted = sorted(dtw_dists, key=(lambda x: x[1]))\n",
    "        euclid_set = set()\n",
    "        dtw_set = set()\n",
    "        for i in range(nNeighbor):\n",
    "            euclid_set.add(euclidean_dists_sorted[i][0])\n",
    "            dtw_set.add(dtw_dists_sorted[i][0])\n",
    "        count = 0\n",
    "        for x in euclid_set:\n",
    "            if x in dtw_set:\n",
    "                count += 1\n",
    "        result.append(float(count)/nNeighbor)\n",
    "    return reduce(lambda x, y: x + y, result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perform = evaluateFunc(100, test_set_rescale, test_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3081\n"
     ]
    }
   ],
   "source": [
    "print perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
